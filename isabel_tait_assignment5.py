# -*- coding: utf-8 -*-
"""Isabel_Tait_Assignment5

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1K8mq9ZScKYQHEpADc9OWlKTr_2-DkmfQ

Isabel Tait
Z23426504
Assignment 5
https://colab.research.google.com/drive/1K8mq9ZScKYQHEpADc9OWlKTr_2-DkmfQ
"""

import tensorflow.keras
from tensorflow.keras.datasets import cifar10

import numpy as np 
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D
from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, BatchNormalization
from tensorflow.keras import optimizers
import matplotlib.pyplot as plt
from tensorflow.keras.callbacks import ModelCheckpoint

# The data, split between train and test sets:
(x_train, y_train), (x_test, y_test) = cifar10.load_data()
print('x_train shape:', x_train.shape)
print(x_train.shape[0], 'train samples')
print(x_test.shape[0], 'test samples')

#splitting validation set 

x_valid=x_train[:int(0.2*x_train.shape[0])]
y_valid=y_train[:int(0.2*x_train.shape[0])]

x_train_new=x_train[int(0.2*x_train.shape[0]):] #reforming training set to not include validation set 
y_train_new=y_train[int(0.2*x_train.shape[0]):]

print(len(x_valid))
print(len(x_train_new))

#print(x_train_new[1])

#normalize the image 
x_train_01 = x_train_new/255
x_test_01 = x_test/255
x_valid_01=x_valid/255

#print(x_train_01[10])

#flatten 
flat_x_train= x_train_01.flatten().reshape(len(x_train_01), 3072)
flat_x_test=x_test_01.flatten().reshape(len(x_test_01), 3072)
flat_x_valid=x_valid_01.flatten().reshape(len(x_valid_01), 3072)

#Convert the label vectors for all the sets to binary class matrices
y_train_binary = tensorflow.keras.utils.to_categorical(y_train_new, num_classes=10)
y_test_binary = tensorflow.keras.utils.to_categorical(y_test, num_classes=10)
y_valid_binary=tensorflow.keras.utils.to_categorical(y_valid, num_classes=10)

print(y_valid[0])
print(y_valid_binary[0])

#Now we have (flat_x_train, y_train),  (flat_x_test, y_test),  (flat_x_valid, y_valid).

#Build FC neural network with 3 layers with 3072 nodes in 1st layer, 4096 nodes in second layer, and 1024 nodes in the third layer 

model = Sequential()
model.add(Dense(4096, input_dim=3072, activation='relu'))
model.add(Dense(1024, activation='relu'))
model.add(Dense(10, activation='softmax')) #Output layer 


#compile and train 

adam = optimizers.Adam(lr=0.001)
model.compile(loss='categorical_crossentropy',
              optimizer=adam,
              metrics=['accuracy'])

#train for 50 epochs with a batch size of 16
history=model.fit(flat_x_train, y_train_binary, batch_size=16, epochs=50, validation_data=(flat_x_valid, y_valid_binary))

#Plot training and validation loss 
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'val'], loc='upper left')
plt.show()

model.summary()

#This network will have a larger number of nodes, which should improve model due to more precise feature extraction 
#Regularization will be added because it prevents over-fitting, for which one regularization technique is using dropout layer


#compile and train 
better_model = Sequential()
better_model.add(Dense(5000, input_dim = 3072, activation = 'relu'))
better_model.add(BatchNormalization())
better_model.add(Dense(2000, activation = 'relu'))
better_model.add(BatchNormalization())
better_model.add(Dropout(0.3))
better_model.add(Dense(600, activation = 'relu'))
better_model.add(BatchNormalization())
better_model.add(Dense(300, activation = 'relu'))
better_model.add(BatchNormalization())
better_model.add(Dropout(0.3))
better_model.add(Dense(10, activation = 'softmax'))

adam = optimizers.Adam(lr=0.001)
better_model.compile(loss='categorical_crossentropy',
              optimizer=adam,
              metrics=['accuracy'])

#train for 50 epochs with a batch size of 16
history2=better_model.fit(flat_x_train, y_train_binary, batch_size=16, epochs=50, validation_data=(flat_x_valid, y_valid_binary))

#Plot training and validation loss 
plt.plot(history2.history['loss'])
plt.plot(history2.history['val_loss'])
plt.title('model 2 loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'val'], loc='upper left')
plt.show()

results = model.evaluate(flat_x_test, y_test_binary, batch_size=64)
print('Testing Loss, Testing Accuracy:', results)

"""Question 4"""

#splitting validation set 

x_valid_CNN=x_train[:int(0.2*x_train.shape[0])]
y_valid_CNN=y_train[:int(0.2*x_train.shape[0])]

x_train_CNN=x_train[int(0.2*x_train.shape[0]):] #reforming training set to not include validation set 
y_train_CNN=y_train[int(0.2*x_train.shape[0]):]

print(len(x_valid_CNN))
print(len(x_train_CNN))

#normalize the images

x_train_CNN01 = x_train_CNN/255
x_test_CNN01 = x_test/255
x_valid_CNN01=x_valid_CNN/255

#print(x_train_CNN01[10])

#Convert the label vectors for all the sets to binary class matrices
y_train_binaryCNN = tensorflow.keras.utils.to_categorical(y_train_CNN, num_classes=10)
y_test_binaryCNN = tensorflow.keras.utils.to_categorical(y_test, num_classes=10)
y_valid_binaryCNN=tensorflow.keras.utils.to_categorical(y_valid_CNN, num_classes=10)

CNN_model = Sequential()
CNN_model.add(Conv2D(32,kernel_size=3, activation='relu', input_shape=x_train_CNN01.shape[1:]))
CNN_model.add(MaxPooling2D(pool_size=(2,2)))
CNN_model.add(Conv2D(64, kernel_size=3, activation='relu'))
CNN_model.add(MaxPooling2D(pool_size=(2,2)))

CNN_model.add(Flatten())
CNN_model.add(Dense(512, activation='relu'))
CNN_model.add(Dropout(0.25))
CNN_model.add(Dense(10, activation='softmax'))


adam = optimizers.Adam(lr=0.001)

CNN_model.compile(loss='categorical_crossentropy',
              optimizer=adam,
              metrics=['accuracy'])


CNN_history=CNN_model.fit(x_train_CNN01, y_train_binaryCNN, batch_size=16, epochs=50, validation_data=(x_valid_CNN01, y_valid_binaryCNN))

#Plot training and validation loss 
plt.plot(CNN_history.history['loss'])
plt.plot(CNN_history.history['val_loss'])
plt.title('model 2 loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'val'], loc='upper left')
plt.show()

results_CNN = CNN_model.evaluate(x_test_CNN01, y_test_binaryCNN, batch_size=64)
print('Testing Loss, Testing Accuracy:', results_CNN)

CNN_model.summary()

"""g) This model is overfitting because the value for the training loss is low, however the value for the validation loss is significantly higher than that of the training loss."""

checkpoint = ModelCheckpoint('model-{epoch:03d}-{accuracy:03f}-{val_accuracy:03f}.h5', verbose=1, monitor='val_loss',save_best_only=True, mode='auto') 

#adam = optimizers.Adam(lr=0.001)

#better_model.compile(loss='categorical_crossentropy',optimizer=adam, metrics=['accuracy'])
bestModel=CNN_model.fit(x_train_CNN01, y_train_binaryCNN, batch_size=16, epochs=50, validation_data=(x_valid_CNN01, y_valid_binaryCNN), callbacks=[checkpoint])

#Plot training and validation loss 
plt.plot(bestModel.history['loss'])
plt.plot(bestModel.history['val_loss'])
plt.title('model 2 loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'val'], loc='upper left')
plt.show()

"""i) The validation loss in step e is lower than that of step h, The accuracy values in step e steadily increased, while the accuruacy values in step h stay the same for almost all epochs. In step h, which uses ModelCheckPoint function, save_best_only is set to True, so the best model is not overwritten, which can explain why the training accuracy is relatively high for all epochs. However, the model may be very overfitted due to how much higher of a value the validation loss is compared to that of the training loss."""

results_bestmodel = CNN_model.evaluate(x_test_CNN01, y_test_binaryCNN, batch_size=64)
print('Testing Loss, Testing Accuracy:', results_bestmodel)
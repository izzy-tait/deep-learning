# -*- coding: utf-8 -*-
"""Isabel_Tait_Assignment 4.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1G2nRExtdcYPB-N103KMoaC1_flqj3DQb

Isabel Tait 
Z23426504
Assignment 4 
https://colab.research.google.com/drive/1G2nRExtdcYPB-N103KMoaC1_flqj3DQb
"""

import matplotlib.pyplot as plt

x_train=[[0,0], [0,1], [1,0], [1,1]]
labels=[0,1,1,0]

for i in range (4):
 if labels[i]==0:
  plt.scatter(x_train[i][0],x_train[i][1],c="blue")
 else: 
  plt.scatter(x_train[i][0],x_train[i][1],c="red", marker="v")

"""c)An XOR gate operation needs a minimum of two layers for it to be implemented, one for each of the two classifier lines needed to accurately classify data points. The minimum number of nodes it would need is 3: 2 nodes in the hidden layer and one in the output layer."""

from tensorflow.keras.models import Sequential 
from tensorflow.keras.layers import Dense
from tensorflow.keras.optimizers import SGD 
import numpy as np


# the four different states of the XOR gate
training_data = np.array([[0,0],[0,1],[1,0],[1,1]])

# the four expected results in the same order
target_data = np.array([[0],[1],[1],[0]])

model=Sequential()
model.add(Dense(2, input_dim=2, activation='relu'))
model.add(Dense(1, activation='sigmoid'))

#model.summary()

sgd=SGD(lr=0.1)

model.compile(loss='mean_squared_error',
              optimizer=sgd,
              metrics=['accuracy'])

history=model.fit(training_data, target_data, batch_size=1, epochs=200, verbose=2)

weights=model.get_weights()
print("These are the weights \n", weights)



acc_curve=np.array(history.history['accuracy'])
loss_curve=np.array(history.history['loss'])
plt.plot(loss_curve)
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

x_train=[[0,0], [0,1], [1,0], [1,1]]
labels=[0,1,1,0]

for i in range (4):
 if labels[i]==0:
  plt.scatter(x_train[i][0],x_train[i][1],c="blue")
 else: 
  plt.scatter(x_train[i][0],x_train[i][1],c="red", marker="v")

x1 = np.array(range(2))
y1 = -(weights[3][0]/weights[2][0][0] + weights[1][0] + weights[0][0][0]*x1)/weights[0][1][0]
plt.plot(x1,y1)

y2 = -(weights[3][0]/weights[2][1][0] + weights[1][1] + weights[0][0][1]*x1)/weights[0][1][1]
plt.plot(x1,y2)
plt.show()

# the four different states of the XOR gate
training_data2 = np.array([[0,0],[0,1],[1,0],[1,1]])

# the four expected results in the same order
target_data2 = np.array([[0],[1],[1],[0]])

model2=Sequential()
model2.add(Dense(4, input_dim=2, activation='relu'))
model2.add(Dense(1, activation='sigmoid'))

#model.summary()

sgd=SGD(lr=0.1)

model2.compile(loss='mean_squared_error',
              optimizer=sgd,
              metrics=['accuracy'])

history=model2.fit(training_data2, target_data2, batch_size=1, epochs=400, verbose=2)

weights2=model2.get_weights()
print("These are the weights \n", weights2)



acc_curve=np.array(history.history['accuracy'])
loss_curve=np.array(history.history['loss'])
plt.plot(loss_curve)
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()

x_train=[[0,0], [0,1], [1,0], [1,1]]
labels=[0,1,1,0]

for i in range (4):
 if labels[i]==0:
  plt.scatter(x_train[i][0],x_train[i][1],c="blue")
 else: 
  plt.scatter(x_train[i][0],x_train[i][1],c="red", marker="v")

x1 = np.array(range(2))
y1 = -(weights2[3][0]/weights2[2][0][0] + weights2[1][0] + weights2[0][0][0]*x1)/weights2[0][1][0]
plt.plot(x1,y1)

y2 = -(weights2[3][0]/weights2[2][1][0] + weights2[1][1] + weights2[0][0][1]*x1)/weights2[0][1][1]
plt.plot(x1,y2)
plt.show()

"""j) The behavior observed is that the classifier lines do not correctly classify the data points whatsoever in the model with double the nodes. However, the classify lines correctly classify data in the first model with only two nodes."""

from keras.datasets import mnist
import random
import numpy as np
from keras.utils import to_categorical
import matplotlib.pyplot as plt
from tensorflow.keras.models import Sequential 
from tensorflow.keras.layers import Dense
from tensorflow.keras.optimizers import SGD

(x_train, y_train), (x_test, y_test) = mnist.load_data()

#1 LAYER 10 NODES MODEL

featureVector=[]
y=[]

featureVectorTest=[]
yTest=[]

positions = np.where(y_train<=2)

for i in positions[0]:
  quad1= np.mean(x_train[i][0:14,14:28])
  quad2=np.mean(x_train[i][14:28,14:28])
  quad3=np.mean(x_train[i][0:14,0:14])
  quad4=np.mean(x_train[i][14:28,0:14])
  featureVector.append([quad1,quad2,quad3,quad4])
  y.append(y_train[i])


positionsTest=np.where(y_test<=2)

for i in positionsTest[0]:
  quad1= np.mean(x_test[i][0:14,14:28])
  quad2=np.mean(x_test[i][14:28,14:28])
  quad3=np.mean(x_test[i][0:14,0:14])
  quad4=np.mean(x_test[i][14:28,0:14])
  featureVectorTest.append([quad1,quad2,quad3,quad4])
  yTest.append(y_test[i])


y_new_train_binary= to_categorical(y, num_classes=3, dtype='float32')
y_new_test_binary=to_categorical(yTest, num_classes=3, dtype='float32')

x=np.array(featureVector)
y=np.array(y_new_train_binary)

xTest=np.array(featureVectorTest)
yTest=np.array(yTest)


model=Sequential()
model.add(Dense(10, input_dim=4, activation='relu'))
model.add(Dense(1, activation='sigmoid'))

sgd=SGD(lr=0.0001)

model.compile(loss='mean_squared_error',
              optimizer=sgd,
              metrics=['acc'])

history=model.fit(x, y, validation_set=0.2, batch_size=16, epochs=50, verbose=2)


plt.plot(history.history['loss'])
plt.show()


# Evaluate the model on the test data using `evaluate`
print('\n# Evaluate on test data')
results = model.evaluate(xTest, yTest, batch_size=16)
print('test loss, test acc:', results)

#1 LAYER 50 NODES MODEL


featureVector=[]
y=[]

featureVectorTest=[]
yTest=[]

positions = np.where(y_train<=2)

for i in positions[0]:
  quad1= np.mean(x_train[i][0:14,14:28])
  quad2=np.mean(x_train[i][14:28,14:28])
  quad3=np.mean(x_train[i][0:14,0:14])
  quad4=np.mean(x_train[i][14:28,0:14])
  featureVector.append([quad1,quad2,quad3,quad4])
  y.append(y_train[i])


positionsTest=np.where(y_test<=2)

for i in positionsTest[0]:
  quad1= np.mean(x_test[i][0:14,14:28])
  quad2=np.mean(x_test[i][14:28,14:28])
  quad3=np.mean(x_test[i][0:14,0:14])
  quad4=np.mean(x_test[i][14:28,0:14])
  featureVectorTest.append([quad1,quad2,quad3,quad4])
  yTest.append(y_test[i])


y_new_train_binary= to_categorical(y, num_classes=3, dtype='float32')
y_new_test_binary=to_categorical(yTest, num_classes=3, dtype='float32')

x=np.array(featureVector)
y=np.array(y_new_train_binary)

xTest=np.array(featureVectorTest)
yTest=np.array(yTest)


model=Sequential()
model.add(Dense(50, input_dim=4, activation='relu'))
model.add(Dense(1, activation='sigmoid'))

sgd=SGD(lr=0.0001)

model.compile(loss='mean_squared_error',
              optimizer=sgd,
              metrics=['acc'])

history=model.fit(x, y, validation_set=0.2, batch_size=16, epochs=50, verbose=2)


plt.plot(history.history['loss'])
plt.show()


# Evaluate the model on the test data using `evaluate`
print('\n# Evaluate on test data')
results = model.evaluate(xTest, yTest, batch_size=16)
print('test loss, test acc:', results)

#1 LAYER 100 NODES MODEL


featureVector=[]
y=[]

featureVectorTest=[]
yTest=[]

positions = np.where(y_train<=2)

for i in positions[0]:
  quad1= np.mean(x_train[i][0:14,14:28])
  quad2=np.mean(x_train[i][14:28,14:28])
  quad3=np.mean(x_train[i][0:14,0:14])
  quad4=np.mean(x_train[i][14:28,0:14])
  featureVector.append([quad1,quad2,quad3,quad4])
  y.append(y_train[i])


positionsTest=np.where(y_test<=2)

for i in positionsTest[0]:
  quad1= np.mean(x_test[i][0:14,14:28])
  quad2=np.mean(x_test[i][14:28,14:28])
  quad3=np.mean(x_test[i][0:14,0:14])
  quad4=np.mean(x_test[i][14:28,0:14])
  featureVectorTest.append([quad1,quad2,quad3,quad4])
  yTest.append(y_test[i])


y_new_train_binary= to_categorical(y, num_classes=3, dtype='float32')
y_new_test_binary=to_categorical(yTest, num_classes=3, dtype='float32')

x=np.array(featureVector)
y=np.array(y_new_train_binary)

xTest=np.array(featureVectorTest)
yTest=np.array(yTest)


model=Sequential()
model.add(Dense(100, input_dim=4, activation='relu'))
model.add(Dense(1, activation='sigmoid'))

sgd=SGD(lr=0.0001)

model.compile(loss='mean_squared_error',
              optimizer=sgd,
              metrics=['acc'])

history=model.fit(x, y, validation_set=0.2, batch_size=16, epochs=50, verbose=2)


plt.plot(history.history['loss'])
plt.show()


# Evaluate the model on the test data using `evaluate`
print('\n# Evaluate on test data')
results = model.evaluate(xTest, yTest, batch_size=16)
print('test loss, test acc:', results)

#2 LAYERS, 100 NODES, 10 NODES MODEL


featureVector=[]
y=[]

featureVectorTest=[]
yTest=[]

positions = np.where(y_train<=2)

for i in positions[0]:
  quad1= np.mean(x_train[i][0:14,14:28])
  quad2=np.mean(x_train[i][14:28,14:28])
  quad3=np.mean(x_train[i][0:14,0:14])
  quad4=np.mean(x_train[i][14:28,0:14])
  featureVector.append([quad1,quad2,quad3,quad4])
  y.append(y_train[i])


positionsTest=np.where(y_test<=2)

for i in positionsTest[0]:
  quad1= np.mean(x_test[i][0:14,14:28])
  quad2=np.mean(x_test[i][14:28,14:28])
  quad3=np.mean(x_test[i][0:14,0:14])
  quad4=np.mean(x_test[i][14:28,0:14])
  featureVectorTest.append([quad1,quad2,quad3,quad4])
  yTest.append(y_test[i])


y_new_train_binary= to_categorical(y, num_classes=3, dtype='float32')
y_new_test_binary=to_categorical(yTest, num_classes=3, dtype='float32')

x=np.array(featureVector)
y=np.array(y_new_train_binary)

xTest=np.array(featureVectorTest)
yTest=np.array(yTest)


model=Sequential()
model.add(Dense(100, input_dim=4, activation='relu'))
model.add(Dense(10, activation='sigmoid'))
model.add(Dense(1, activation='sigmoid'))

sgd=SGD(lr=0.0001)

model.compile(loss='mean_squared_error',
              optimizer=sgd,
              metrics=['acc'])

history=model.fit(x, y, validation_set=0.2, batch_size=16, epochs=50, verbose=2)


plt.plot(history.history['loss'])
plt.show()


# Evaluate the model on the test data using `evaluate`
print('\n# Evaluate on test data')
results = model.evaluate(xTest, yTest, batch_size=16)
print('test loss, test acc:', results)

#2 LAYERS, 100 NODES, 50 NODES MODEL


featureVector=[]
y=[]

featureVectorTest=[]
yTest=[]

positions = np.where(y_train<=2)

for i in positions[0]:
  quad1= np.mean(x_train[i][0:14,14:28])
  quad2=np.mean(x_train[i][14:28,14:28])
  quad3=np.mean(x_train[i][0:14,0:14])
  quad4=np.mean(x_train[i][14:28,0:14])
  featureVector.append([quad1,quad2,quad3,quad4])
  y.append(y_train[i])


positionsTest=np.where(y_test<=2)

for i in positionsTest[0]:
  quad1= np.mean(x_test[i][0:14,14:28])
  quad2=np.mean(x_test[i][14:28,14:28])
  quad3=np.mean(x_test[i][0:14,0:14])
  quad4=np.mean(x_test[i][14:28,0:14])
  featureVectorTest.append([quad1,quad2,quad3,quad4])
  yTest.append(y_test[i])


y_new_train_binary= to_categorical(y, num_classes=3, dtype='float32')
y_new_test_binary=to_categorical(yTest, num_classes=3, dtype='float32')

x=np.array(featureVector)
y=np.array(y_new_train_binary)

xTest=np.array(featureVectorTest)
yTest=np.array(yTest)


model=Sequential()
model.add(Dense(100, input_dim=4, activation='relu'))
model.add(Dense(50, activation='sigmoid'))
model.add(Dense(1, activation='sigmoid'))

sgd=SGD(lr=0.0001)

model.compile(loss='mean_squared_error',
              optimizer=sgd,
              metrics=['acc'])

history=model.fit(x, y, validation_set=0.2, batch_size=16, epochs=50, verbose=2)


plt.plot(history.history['loss'])
plt.show()


# Evaluate the model on the test data using `evaluate`
print('\n# Evaluate on test data')
results = model.evaluate(xTest, yTest, batch_size=16)
print('test loss, test acc:', results)

"""The training loss seems to be a bit lower the more nodes there are and the more layers there are. Though, all models have very similar training losses. Judging by the graphs, it seems the last two are the most suitable models. They both have 2 layers with 100 nodes in the first layer."""